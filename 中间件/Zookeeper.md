# Zookeeper

[TOC]

## what

ZooKeeper主要服务于分布式系统，可以用ZooKeeper来做：注册中心、配置中心、分布式锁。

zookeeper内部是一个树状的文件结构，树的每个节点称为znode，这些节点和我们普通的目录一样可以新建、删除、修改。

Znode主要分为两种类型：

    1. 短暂/临时(Ephemeral)：当客户端和服务端断开连接后，所创建的Znode会自动删除
    2. 持久(Persistent)：当客户端和服务端断开连接后，所创建的Znode不会删除

znode节点的状态信息中包含**zxid**，它是当前节点最新的事务ID，ID值越大，则说明数据越新。

## 持久化

TxnLog：增量事务日志，记录系统中所有的增删改记录
SnapShot ：快照，记录内存中的全量数据，但是数据不一定是最新的


## 集群中的几种角色

- Leader：Leader是由选举产生的

  作为事务的协调者，发起与提交写请求

  与learner（ follower 和 obverser ）保持心跳

  崩溃恢复时负责恢复数据以及同步数据到Learner

- Follower：直接为客户端服务并参与提案的投票，同时与 Leader 进行数据交换

- Observer：直接为客户端服务但不参与提案的投票，同时也与 Leader 进行数据交换。主要作用是提高zookeeper集群的读性能。


## CAP理论解释Zookeeper

C（一致性）: Zookeeper通过paxos算法保证了顺序一致性（满足最终一致性），一半以上的节点数据是最新的，并且在十几秒可以Sync到各个节点，如果想保证取得是数据一定是最新的，需要手工调用Sync()

A（可用性）: Zookeeper只要一半以上的节点正常就能对外提供服务，因为选举和数据更新都需要一半以上的节点投票通过，但是它的可用性并不完美，选举期间不可用，节点越多选举越慢，zookeeper引入了观察者缓解了这个问题

P（分区容错性）:一份数据在多个节点保存，当发生网络分区时，仍然能访问到数据



## zab算法

分布式一致性算法

1. 保证全局有序
2. 保证事务被提交


## 数据同步

zookeeper 的数据同步是为了保证每个节点的数据一致性，大致分为 2 个流程：

一个是正常的客户端数据提交流程；

二是集群中某个节点宕机后数据恢复流程。

### 集群写请求过程（2PC）

Leader节点，针对当前请求生成日志（Txn）
Leader节点，持久化前请求生成日志（Txn），并向自己发送一个Ack
Leader节点，把当前请求生成的日志（Txn）发送给其他所有的参与者节点（非Observer）
Leader节点，阻塞等待Follower节点发送Ack过来（超过一半则解阻塞）
Follower节点，接收到Leader节点发送过来的Txn
Follower节点，持久化当前Txn，并向Leader节点发送一个Ack
Leader节点，接收到了超过一半的Ack（加上自己发给自己的Ack,），则解阻塞。这点和二段提交有点不同，二段提交要求收到所有的ack。
Leader节点，向Follower节点发送commit命令（异步发送的，不会阻塞Leader节点）
Leader节点，执行Txn，更新内存（根据Txn更新DataBase）
Follower节点，接收到Leader节点发送过来的commit命令
Follower节点，执行Txn，更新内存（根据Txn更新DataBase）

### 崩溃恢复

1. 已经被提交的提案不能被丢弃

    **假设：**

    如果在leader发出了commit之后，各个follow收到commit之前，leader挂掉了，导致follow并没有执行已经提交的提案。这个时候，这个消息是不能丢失的；

    **解决方案：**

     leader失效后，重新选举出来的leader肯定具备最大的zxid（不考虑这个zxid有没有被提交），只要zxid最大，那么就会被选为leader（myid也得考虑，这里不是重点），zxid最大说明这个节点肯定包括了所有的最新的提案，当这个节点当选为leader之后，新的leader会检查自身有没有未被提交的提案，如果有的，则会向集群中发送请求，询问其他follow节点是否存在其提案，如果超过半数回复ok,则执行提交操作，之后进行数据同步操作，这样就保证了已经被提交的提案不会被丢失。

2. 没有被提交的提案应该被丢弃

    **假设：**

    如果在leader生成提案后，广播之前，leader崩溃了，这个时候的提案应该被丢弃。

    **解决方案：**

    zab算法通过zxid来实现这一目的

    zxid占据64位，高32位存储epoch编号，epoch每选举出一次leader之后都会加一低32位从0开始，

    低32位用于递增计数，当有新的请求或出现新的提案时，就会加1，但是重新选择leader之后，就会进行清零； 

    在旧的leader重启后，因为已经经过一次新的选举了，旧的leader所处的朝代已经落后了，新的leader会要求旧的leader将 它所处的朝代没有被提交 的提案清除，重新同步最新的提案，这就保证了未被提交的提案进行丢弃；


## 节点选举

暂停服务，每个服务器提交自己作为leader的提案，每个通过比较zxid大小投票，半数以上通过即选为leader。


## 缺点

1. 选举时间过长

2. 只能保证一半以上的数据同步

3. ZK集群不支持在线动态添加机器或替换机器

4. 就算扩容，也解决不了 ZooKeeper 写性能的问题，ZooKeeper 写是不可扩展的，并且应用发布时有大量的请求排队，从而使得接口性能急剧下降，表现出来的现象就是应用启动十分缓慢。


## 应用场景

- 注册中心

提供者、消费者都向Zookeeper注册自己的URL，消费者通过服务提供者的URL调用服务。而提供者发生了变动，也会通过Zookeeper向订阅的消费者发送通知。

- 配置中心

这个主要用到了zookeeper的watch机制，对节点监听

- 分布式锁

哪个节点先创建节点，则谁获得锁

## FAQ

- ZooKeeper集群中服务器之间是怎样通信的？

  Leader服务器会和每一个Follower/Observer服务器都建立TCP连接，同时为每个F/O都创建一个叫做LearnerHandler的实体。

  LearnerHandler主要负责Leader和F/O之间的网络通讯，包括数据同步，请求转发和Proposal提议的投票等。Leader服务器保存了所有F/O的LearnerHandler。

- 为什么要半数以上确认？

  崩溃后数据不丢失

- 为什么服务器数量要求奇数个？

  如果是偶数个，脑裂后服务器数量就有可能相等，无法选出leader。